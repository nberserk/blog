---
layout: post
title: Parquet + Avro + Spark
tags: parquet
date: 2015-12-09
---

<p>
데이터 분석을 위해 파일을 저장해야 할 필요가 있었다. 처음에는 csv파일 형식으로 저장을 했는데, 시간이 지남에 따라서 새로운 컬럼이 생기는 요구사항이 생겼다. 이런 경우 csv는 어떤 정보가 몇번째 컬럼에 있는지를 기술하지 않기 때문에 또 다른 파일에 컬럼 정보를 기록하고 데이터 타입등도 기술을 해줘야 하는 불편함이 생긴다. 
언뜻 parquet이 그런 일을 하는것이라 어렴풋이 알고 있었기 때문에 이번에 parquet을 적용해 볼겸 조사를 해봤다.
</p>

<p>
특징들..
</p>
<ul class="org-ul">
<li>압축 지원. 50% 정도 세이브 할 수 있다고 함. 스토리지 비용이 반이라는 얘기.</li>
<li>여러가지 serialize framework 지원 (Avro, Thrift, protocol buffer)</li>
</ul>

<div id="outline-container-orgheadline1" class="outline-2">
<h2 id="orgheadline1">column based</h2>
<div class="outline-text-2" id="text-orgheadline1">
<p>
columnar storage다. 이렇게 접근한 이유는 크게 2가지.
</p>

<p>
보통 쿼리를 할때 모든 컬럼의 정보가 다 필요한 경우는 많지 않다. row based 라면 전체 열을 다 읽어야 쿼리를 수행할 수 있지만, parquet는 필요한 컬럼만 로드하면 된다. 여기서 속도 향샹이 생긴다.
</p>

<p>
그리고 컬럼끼리 모아서 압축을 하면 압축률이 더 좋아진다. timestamp를 가지는 컬럼이라고 생각하면 델터 인코딩 방식으로 압축을 하면 좋을 것이고, 각 컬럼의 특징이 살아 있으니 더 유리하다.
</p>
<iframe src="//www.slideshare.net/slideshow/embed_code/key/jKNTsYfuHHgao?startSlide=33" width="595" height="485" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;" allowfullscreen> </iframe>
<p>
2G의 원본 csv를 SNAPPY압축을 이용해서 저장하니 1G로 50%정도나 줄어들었다.
</p>

<p>
그리고 query performance도 일반 텍스트에 비해서 2배 정도 성능이 좋다고 한다.
</p>
<iframe src="//www.slideshare.net/slideshow/embed_code/key/jKNTsYfuHHgao?startSlide=34" width="595" height="485" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;" allowfullscreen> </iframe>
</div>
</div>

<div id="outline-container-orgheadline2" class="outline-2">
<h2 id="orgheadline2">단점</h2>
<div class="outline-text-2" id="text-orgheadline2">
<p>
마냥 좋기만 한것은 아니다. 분석용으로 최고의 파일 형식이라고 한 것은 데이터의 업데이트가 없다는 뜻이다. 즉, readonly일때 좋은 것이다. 
</p>
</div>
</div>

<div id="outline-container-orgheadline3" class="outline-2">
<h2 id="orgheadline3">환경 설정</h2>
<div class="outline-text-2" id="text-orgheadline3">
<p>
spark + parquet + avro 를 사용하려면 다음과 같은 디펜던시가 필요하다. 
</p>
<div class="org-src-container">

<pre class="src src-xml">&lt;properties&gt;
  &lt;scala.version&gt;2.11.4&lt;/scala.version&gt;
  &lt;maven.compiler.source&gt;1.7&lt;/maven.compiler.source&gt;
  &lt;maven.compiler.target&gt;1.7&lt;/maven.compiler.target&gt;
  &lt;parquet.version&gt;1.8.1&lt;/parquet.version&gt;
  &lt;avro.version&gt;1.7.7&lt;/avro.version&gt;
&lt;/properties&gt;

&lt;dependencies&gt;
  &lt;dependency&gt;
    &lt;groupId&gt;org.apache.parquet&lt;/groupId&gt;
    &lt;artifactId&gt;parquet-common&lt;/artifactId&gt;
    &lt;version&gt;${parquet.version}&lt;/version&gt;
  &lt;/dependency&gt;
  &lt;dependency&gt;
    &lt;groupId&gt;org.apache.parquet&lt;/groupId&gt;
    &lt;artifactId&gt;parquet-encoding&lt;/artifactId&gt;
    &lt;version&gt;${parquet.version}&lt;/version&gt;
  &lt;/dependency&gt;
  &lt;dependency&gt;
    &lt;groupId&gt;org.apache.parquet&lt;/groupId&gt;
    &lt;artifactId&gt;parquet-column&lt;/artifactId&gt;
    &lt;version&gt;${parquet.version}&lt;/version&gt;
  &lt;/dependency&gt;
  &lt;dependency&gt;
    &lt;groupId&gt;org.apache.parquet&lt;/groupId&gt;
    &lt;artifactId&gt;parquet-hadoop&lt;/artifactId&gt;
    &lt;version&gt;${parquet.version}&lt;/version&gt;
  &lt;/dependency&gt;

  &lt;dependency&gt;
    &lt;groupId&gt;org.apache.parquet&lt;/groupId&gt;
    &lt;artifactId&gt;parquet-avro&lt;/artifactId&gt;
    &lt;version&gt;${parquet.version}&lt;/version&gt;
  &lt;/dependency&gt;

  &lt;dependency&gt;
    &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;
    &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt;
    &lt;version&gt;1.1.0&lt;/version&gt;
    &lt;scope&gt;provided&lt;/scope&gt;
  &lt;/dependency&gt;
&lt;/dependencies&gt;

&lt;dependency&gt;
  &lt;groupId&gt;org.apache.avro&lt;/groupId&gt;
  &lt;artifactId&gt;avro&lt;/artifactId&gt;
  &lt;version&gt;${avro.version}&lt;/version&gt;
&lt;/dependency&gt;
</pre>
</div>
</div>
</div>

<div id="outline-container-orgheadline4" class="outline-2">
<h2 id="orgheadline4">avro schema define</h2>
<div class="outline-text-2" id="text-orgheadline4">
<p>
자세한 사항은 <a href="http://avro.apache.org/docs/1.7.7/spec.html#schemas">avro spec </a>을 보면 되고, 아래처럼 정의 하면 된다. 이 파일을 avro-tools.jar를 이용하면 POJO class를 만들 수 있고 이 파일을 이용하면 프로그래밍이 조금더 이뻐질 수 있다. 아래 read/save 예제에서 User class를 사용하는데 이것이 스키마를 바탕으로 생성된 클래스이다. 필수는 아니고 POJO class가 없을때는 GenericRecord를 사용할 수 도 있다.
</p>

<div class="org-src-container">

<pre class="src src-json">{
    "namespace": "com.nberserk.example.avro",
    "type": "record",
    "name": "User",
    "fields": [
        {"name": "id", "type": "string"},
        {"name": "age",  "type": "int"},
        {"name": "weight", "type":"float"}
    ]
}
</pre>
</div>
</div>
</div>
<div id="outline-container-orgheadline5" class="outline-2">
<h2 id="orgheadline5">parquet save/read in java</h2>
<div class="outline-text-2" id="text-orgheadline5">
<div class="org-src-container">

<pre class="src src-java">Schema schema = new Schema.Parser().parse(new File("src/test/avro/user.avro"));        
File tmp = new File("test.parquet");
Path path = new Path(tmp.getPath());        

ParquetWriter&lt;GenericRecord&gt; writer = AvroParquetWriter
    .&lt;GenericRecord&gt;builder(path)
    .withSchema(schema)
    .withCompressionCodec(CompressionCodecName.SNAPPY)                
    .build();

// Write a record with GenericRecord
GenericRecord r = new GenericData.Record(schema);
r.put("uid", "darren");
r.put("age", 22);
r.put("weight", 70.0);
writer.write(r);
writer.close();

/*
// write a record using generated POJO class called User
ParquetWriter&lt;User&gt; writer = AvroParquetWriter.&lt;User&gt;builder(path)
.withCompressionCodec(CompressionCodecName.SNAPPY)
.withSchema(schema)
.build();

User p = new Profile();
p.setId("darren");
p.setAge(22);
p.setWeight(70.0);
writer.write(p);
writer.close();
*/
Configuration conf = new Configuration();
AvroReadSupport.setAvroReadSchema(conf, Profile.SCHEMA$);
ParquetReader&lt;Profile&gt; reader = AvroParquetReader.&lt;Profile&gt;builder(path)
    .withConf(conf)
    .build();
Profile p1 = reader.read();

assertEquals("darren", p1.getUid().toString());
assertEquals(22, p1.getAge());
assertEquals(77.0, p1.getWeight());
</pre>
</div>
</div>
</div>

<div id="outline-container-orgheadline6" class="outline-2">
<h2 id="orgheadline6">Spark에서 parquet 읽기</h2>
<div class="outline-text-2" id="text-orgheadline6">
<p>
spark 1.5.0 미만에서는 <a href="https://issues.apache.org/jira/browse/SPARK-6566">parquet로드시 익셉션을 내는 버그가 있었는데</a> 그때는 스파크 버전을 1.5.0 이상으로 올리면 해결 된다.
</p>
<div class="org-src-container">

<pre class="src src-scala">val p = sqlContext.parquetFile("s3://test.parquet")
val multipleParquet = sqlContext.parquetFile("s3://p1", "s3://p2")
</pre>
</div>
</div>
</div>

<div id="outline-container-orgheadline7" class="outline-2">
<h2 id="orgheadline7">schema merge</h2>
<div class="outline-text-2" id="text-orgheadline7">
<p>
스키마가 다른 여러 parquet 파일을 로드할때는 스키마 머지가 필요한데, 스파크 1.5.0 부터는 디폴트로 옵션이 꺼져 있다. 그래서 교집합?만 로드가 된다. 
아래처럼 mergeSchema 옵션을 켜고 하면 합집합(?)으로 로드 된다. 
</p>
<div class="org-src-container">

<pre class="src src-scala">val p = sqlContext.read.option("mergeSchema", "true").parquetFile("s3://v1.parquet", "s3://v2.parquet")
</pre>
</div>
</div>
</div>

<div id="outline-container-orgheadline8" class="outline-2">
<h2 id="orgheadline8">avro schema evolution</h2>
<div class="outline-text-2" id="text-orgheadline8">
<p>
avro를 사용하면 좋은 점은 스키마 변화가 있을때 유연하게 대처할 수 있는 점인데, 아래의 경우를 생각해보자. 
</p>

<p>
Profile은 name필드만 있었는데, v2에서 "create<sub>time</sub>", "newStringNullDefault", "union" 등이 추가 되었다. create<sub>time</sub>, newStringNullDefault은 default값이 있고, union(둘중 하나)은 union으로 선언되어졌다. 이때 Profile 스키마로 저장된 parquet파일을 Profile2로 읽으면 어떻게 될까?
</p>

<p>
default 값을 가진 필드는 디폴트 값이 채워지고, union들은 생략된다. 아래 예제 참고.
</p>

<div class="org-src-container">

<pre class="src src-java">/*

// Profile avro spec
{"namespace": "com.nberserk.avro",
 "type": "record",
 "name": "Profile",
 "fields": [
     {"name": "name", "type": "string"}
 ]
}

// Profile2 avro spec
{"namespace": "com.nberserk.avro",
 "type": "record",
 "name": "Profile2",
 "fields": [
     {"name": "name", "type": "string"}
     {"name": "create_time", "type":"long", "default":0},
     {"name": "union", "type": ["null", "string"]},
     {"name": "value_default", "type": "string", "default": "null"}
 ]
}
*/   
Schema schema = new Schema.Parser().parse(new File("src/test/avro/profile.avro"));

File tmp = new File("test2.tmp");
tmp.deleteOnExit();
tmp.delete();

Path file = new Path(tmp.getPath());
ParquetWriter&lt;GenericRecord&gt; writer = AvroParquetWriter
    .&lt;GenericRecord&gt;builder(file)
    .withSchema(schema)
    .build();

// Write a record with a null value
GenericRecord r = new GenericData.Record(schema);
for (int i = 0; i &lt; 100; i++) {
    r.put("name", "darren");
    writer.write(r);
}
writer.close();


// Let's load the same file with v2 schema
Schema v2 = new Schema.Parser().parse(new File("src/test/avro/profile2.avro"));
Configuration conf = new Configuration();
AvroReadSupport.setAvroReadSchema(conf, v2);

ParquetReader&lt;GenericRecord&gt; reader = AvroParquetReader.&lt;GenericRecord&gt;builder(file)
    .withConf(conf).build();
GenericRecord gr = reader.read();

assertEquals("darren", gr.get("name").toString());
assertEquals(0L, gr.get("create_time"));
assertEquals("null", gr.get("newStringNullDefault").toString());
</pre>
</div>
</div>
</div>

<div id="outline-container-orgheadline9" class="outline-2">
<h2 id="orgheadline9">performance comparison</h2>
<div class="outline-text-2" id="text-orgheadline9">
<p>
원본 사이즈, txt로는 20G, parquet로는 6.5G 정도임 row수는 1억1천 row정도의 데이터.
</p>

<p>
file size, 20G 파일이 parquet으로 저장하니 6.5G 정도로 작아짐. 50% 이상.
query, 특정 값을 가진 사용자들의 수를 쿼리하는 것을 각각 측정해보니 txt의 경우는 186초. parquet의 경우는 105초 정도가 소요됨. 
</p>

<p>
따라서, 스토리지 용량/ 쿼리 속도 면에서 모두 만족할 만한 결과를 보임. 
</p>
</div>
</div>

<div id="outline-container-orgheadline11" class="outline-2">
<h2 id="orgheadline11">trouble shooting</h2>
<div class="outline-text-2" id="text-orgheadline11">
</div><div id="outline-container-orgheadline10" class="outline-3">
<h3 id="orgheadline10">writing parquet files to s3 is too slow</h3>
<div class="outline-text-3" id="text-orgheadline10">
<p>
csv에서 parquet파일로 바꾸고 난후에 S3에 write 할때 시간이 너무 오래 걸려서 분석해보니 write.parquet()의 시간이 너무 오래 걸렸다. 검색을 해보니 <a href="http://spark.apache.org/docs/latest/sql-programming-guide.html#configuration">spark.sql.parquet.output.committer.class 을 org.apache.spark.sql.parquet.DirectParquetOutputCommitter 로 설정</a>하면 괜찮아진다는 얘기가 있다. 수정해보니 효과가 있었다. 
</p>

<div class="org-src-container">

<pre class="src src-java">//sc : SparkContext
sc.hadoopConfiguration.set("spark.sql.parquet.output.committer.class", "org.apache.spark.sql.parquet.DirectParquetOutputCommitter")
</pre>
</div>
</div>
</div>
</div>

<div id="outline-container-orgheadline12" class="outline-2">
<h2 id="orgheadline12">revision history</h2>
<div class="outline-text-2" id="text-orgheadline12">
<ul class="org-ul">
<li>2015/12/10 initial draft</li>
<li>2015/12/18 add schema evolution</li>
<li>trouble shooting/s3 slow added</li>
</ul>
</div>
</div>
