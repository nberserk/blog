---
layout: post
title:  "Zeppelin"
tags:
 - zeppelin
---

Zeppelin은 아파치 인큐베이팅 프로젝트로 spark-shell 을 웹에서 할 수 있게 옮겨 놓은 것 + ipython notebook + data visualization(with d3.js) + screen sharing with websocket 정도 되겠다. 게다가 스파크의 변수를 [angular변수로 맵핑](https://www.youtube.com/watch?v=xU5TBS_MsAs)이 가능해서 동적인 화면을 구성할 수 있다.
실제 사용해 보면 유용하고 데이터 분석을 하기에 최적화 되어 있다. 분석과정을 보여줄 수 있어 유용하다. NFLab이라고 하는 한국 스타트업에서 만들었는데 작품인 것 같다. 스파크와 함께 요즘에 젤 눈에 띄는 프로젝트다.

interpreter를 통해서 백엔드랑 통신을 하고 그 결과를 노트북으로 가져와서 보여주는 컨셉. 현재 지원하는 것은 markdown, spark, shell, hive, Tajo 등이 있다. 로드맵상 가능한 분석툴들은 다 지원하려고 하는 것 같다.

## 설치 및 설정
1. 아직 인큐베이팅 프로젝트라서 바이너리는 배포가 안되고 있어서 직접 빌드를 해야 한다. [zeppelin github project](https://github.com/apache/incubator-zeppelin) 에 가면 빌드 방법이 가이드가 되어 있고 나의 경우는 아래처럼 빌드했다.   `mvn clean package -Pspark-1.3 -Dhadoop.version=2.3.0 -Phadoop-2.3 -DskipTests`

2. 빌드가 끝나면 bin/zeppelin-daemon.sh가 생성이 되는데 bin/zeppelin-daemon.sh start하면 실행이 된다.
3. 그러면 localhost:8080으로 접속하면 web interface가 뜬다. 여기서 위의 Interpreter tab을 누르고 spark interperter의 master 주소를 설정하면 끝.
4. 스파크 클러스트 중의 하나의 컴에 제플린 서버를 두는게 설정이 쉽다. 처음엔 외부에 서버를 띄우려고 했었는데 1차 시도는 실패했던 기억

## Tips
1. 특정 jar를 제플린에서 로드하고 싶으면 interpreter tab에서 args property에 다음 처럼 명시해주면 된다. --packages <groupId>:<artifactId>:<version> 이런 형식으로
```
 --packages com.google.code.gson:gson:2.3.1
```

## TroubleShooting
뭐가 잘 안될때 분산되어 동작하는 것 때문에 디버깅이 쉽지 않은데 /logs/ 폴더 하위의 로그들을 잘 보면 힌트가 있다.

#### serialization error
한가지 경험담을 공유하자면 위에서 빌드할때 -Pspark-1.3을 주면 기본적으로 1.3.1으로 빌드를 하는데, 나는 스파크 1.3.0 버전을 설치해서 스파크 클러스터와 제플린 간의 라이브러리 버전이 달라서 실패하는 경우가 있었다. 대부분의 커맨드는 괜찮은데 특정 상황에서 특정 클래스의 Serialize id 가 달라서 로그와 나온게 있었는데 그것으로 인해 추측해서 알게 되었다. 에러는 아래와 같았다.

```
Slf4jLogger.scala[apply$mcV$sp]:71) - Association with remote system [akka.tcp://sparkExecutor@sa-dev-server:57240] has failed, address is now gated for [5000] ms. Reason is: [org.apache.spark.TaskState$; local class incompatible: stream classdesc serialVersionUID = -2913614267616900700, local class serialVersionUID = 746799155515967470].
```

#### YARN support
`Caused by: java.lang.ClassNotFoundException: org.apache.spark.deploy.yarn.YarnSparkHadoopUtil` 이런 에러가 발생하면 -Pyarn 옵션을 줘서 빌드해 주면 됨.

