---
layout: post
title:  "Spark 설치 및 설정"
tags: spark
---

## 환경
- spark 1.3.0 버전 기준임.
- hadoop 2.3.0 기준

## Standalone vs YARN
deploy 방법에는 Standalone 과 YARN 모드가 있다. MESOS는 안해봐서 패스. YARN으로 복수개의 스파크 잡을 스케쥴링 하거나 동시에 실행할 수 있어 프러덕션환경에서는 YARN이 필수라고 볼 수 있겠고 테스트 용도라면 Standalone이 설정하기 쉽다.
예를 들면, Standalone은 실행중일때 다른 스파크 잡이 들어오면 무시되지만, YARN은 그 잡을 스케쥴링 해서 수행하고 있는 잡이 끝나면 실행해 준다. 스파크 클러스터의 사용자가 많아지고 잡이 많아지면 YARN은 필수 되겠다.

## Standalone mode
1. master가 될 컴퓨터에 spark 1.3.0 버전을 다운받아서 압축을 푼다
2. conf/spark-env.sh 에
 3. `SPARK_MASTER_IP` 에 master ip 기입
 4. ~~`SPARK_LOCAL_IP` 에 자신의 ip 기입. 이것은 spark-env.sh보다는 환경변수에 해주는 것이 좋다. IP는 노드마다 다를테니까.~~
2. conf/slaves 에 slave ip 혹은 호스트 명을 기입 - 한줄에 하나씩
3. spark 폴더 전체를 다른 slave에 복사. 앞서 설명한 [bash script]({% post_url 2015-06-11-hadoop-intro %})를 사용하면 쉽다.
4. master에서 .sbin/start-all.sh 실행
5. http://<master ip>:8080/ 에 접속하면 spark web ui를 볼 수 있다.

## YARN
yarn이 편한것은 굳이 spark를 hadoop cluster에 설치하지 않아도 되는 것이다. 그 이유는 yarn에 어플을 submit할때 피룡한 모든 jar를 던져 주기 때문.
해서, spark 다른 버전 여러개를 동시에 yarn 상에서 구동해 볼 수 도 있겠다.

1. ~~master가 될 컴퓨터에 spark 1.3.0 버전을 다운받아서 압축을 푼다.~~
1. spark-submit을 사용할 컴에 스파크를 다운 받아 압축을 풀고
1. `HADOOP_CONF_DIR` 환경변수 설정
2. ~~이 작업을 모든 컴에서 반복.~~
3. spark-submit 을 불러주면 된다.

`spark-shell --master yarn-client`을 실행해서 해보면 잘 동작하는지 확인 할 수 있다.
job submit은 `spark-submit --class --master yarn-cluster xxxx.jar ` 이런식..

## spark on EMR
cluster모드로 이용하면 exception이 발생했을때 디버깅이 여러모로 번거로운데 팁을 정리해 본다.

### EMR log이용 해서 디버깅
EMR 상에서 spark job을 실행시키면 yarn의 arregation-log option이 default로 꺼져 있는데 그래서 log를 보기가 쉽지가 않다. 하지만, EMR의 하둡은 AWS상으로 로그를 남기는 기능이 있어서 더 쉽게 spark job의 로그를 확인할 수 있다. 단, EMR cluster를 실행할때 LogUri를 걸어주기만 하면 끝이다. 그러면 EMR 4.0 에서는 `<bucket_path>/<cluster_id>/container/<application_id>` 하위에 spark로그들이 쌓이므로 이것을 확인하면 디버깅이 가능하다.

`j-3NJPE2Q3IR2T/containers/application_1450245879642_0002/container_1450245879642_0002_01_000001`
위의 예는 <j-3NJPE2Q3IR2T> 의 클러스터에 2번째 스파크 잡의 로그는 위의 경로에 저장된다. driver가 000001의 아이디를 가지고 되나본데(?) 그래서 첫번째의 로그만 보면 보통 해결이 된다.
단점은, 로그가 s3로 싱크가 되려면 10분정도의 시간이 걸리기 때문에 불편하다. 그래서 아래에서 lynx 를 이용하는 부분을 추가했다.

### EMR에서 lynx를 이용해서 디버깅
특정 스파크 잡이 실패하면 보통 아래 처럼 로그가 뜬다. 그러면 아래의 tracking url을 이용해서 shell 기반 브라우져인 lynx를 이용해서 들어가면 로그를 바로 확인할 수 있다. 1.5.0의 경우는 shell에서 `lynx http://ip-xxx.us-west-2.compute.internal:20888/proxy/application_1450245879642_0004` 들어간후 executors 링크를 타고 들어가 driver의 stderr로그를 보면 대부분 해결된다.

```
07:12:27 INFO yarn.Client:
	 client token: N/A
	 diagnostics: N/A
	 ApplicationMaster RPC port: 0
	 queue: default
	 start time: 1450249874517
	 final status: FAILED
	 tracking URL: http://ip-000-00-00-00.us-west-2.compute.internal:20888/proxy/application_1450245879642_0004/history/application_1450245879642_0004/2
	 user: hadoop
Exception in thread "main" org.apache.spark.SparkException: Application application_1450245879642_0004 finished with failed status
	at org.apache.spark.deploy.yarn.Client.run(Client.scala:920)
	at org.apache.spark.deploy.yarn.Client$.main(Client.scala:966)
	at org.apache.spark.deploy.yarn.Client.main(Client.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

```

## enabling spark event log
conf/spark-defaults.conf 를 아래처럼.

```
spark.eventLog.enabled true
spark.eventLog.dir hdfs://xxxx/sparklogs
```

이렇게 하고 spark job을 실행하면 위에서 설정한 hdfs에 spark event log가 쌓이게 된다. 이 이벤트 로그는 spark의 history server를 통해서 볼 수 있다. `sbin/start-history-server.sh` 를 통해서 실행하면 되는데 그전에 로그 파일들이 어디에 있는지 지정을 해줘야 한다. 그것은 아래와 같은 환경 변수로 할 수 있다.

`export SPARK_HISTORY_OPTS=" -Dspark.history.fs.logDirectory=hdfs://xxx/sparklog"`

그후 histgory server를 실행하면, 실행한 컴의 http://<ip>:18080 으로 들어가면 web ui에서 각 잡의 logging을 볼 수 있다.

## history
- 15/8/18 fix wrong info about yarn cluster
- 15/12/15 adding debugging tip on EMR spark
